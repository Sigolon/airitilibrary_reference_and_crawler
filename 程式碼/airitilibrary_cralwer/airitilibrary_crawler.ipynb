{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要\n",
    "此專案目標在於，從華藝網站中，爬取需要論文的作者、網址、摘要等資訊，並組成資料庫，以利後續的利用。\n",
    "預期利用有 : \n",
    "1. 自動化的參考文獻製作。\n",
    "2. 自動化尋找過往研究使用的控制變數，從研究方法的討論出發。\n",
    "3. 自動化統整過往研究的結論，作為後續推論的前提。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_1\n",
    "此處透過遍歷每一個頁面之資訊，蒐集所有的論文的 url ，以利後續對每一個論文網頁，發出請求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, pandas, numpy, traceback, re, os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def os_path(list) : \n",
    "    os_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "    for i in list : \n",
    "        os_path = os.path.join(os_path, i)\n",
    "    return os_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建錯誤資訊資料夾\n",
    "os.makedirs(os_path([\"程式碼\", \"airitilibrary_cralwer\", \"error\", \"error_crawler_step1\"]), mode=0o777, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 華藝搜尋\"社會學\"\n",
    "url = \"https://www.airitilibrary.com/Search/ArticleSearch?ArticlesViewModel_SearchField=%E7%A4%BE%E6%9C%83%E5%AD%B8&ArticlesViewModel_TitleKeywordsAbstract=&ArticlesViewModel_FulltextSearchField=&ArticlesViewModel_Author=&ArticlesViewModel_JournalBookDepartment=&ArticlesViewModel_DOI=&ArticlesViewModel_ArticleArea_Taiwan=false&ArticlesViewModel_ArticleArea_ChinaHongKongMacao=false&ArticlesViewModel_ArticleArea_American=false&ArticlesViewModel_ArticleArea_Other=false&PublicationsViewModel_SearchField=&PublicationsViewModel_PublicationName=&PublicationsViewModel_ISSN=&PublicationsViewModel_PublicationUnitName=&PublicationsViewModel_DOI=&PublicationsViewModel_PublicationArea_Taiwan=false&PublicationsViewModel_PublicationArea_ChinaHongKongMacao=false&PublicationsViewModel_PublicationArea_American=false&PublicationsViewModel_PublicationArea_Other=false\"\n",
    "\n",
    "airitilibrary_get = requests.get(url)\n",
    "print(airitilibrary_get) # <Response [200]>\n",
    "\n",
    "airitilibrary_bs4 = BeautifulSoup(airitilibrary_get.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推測他將每一篇文章的網址、作者 等，都儲存在 <tr>的標籤之中。\n",
    "# 抓出每一個 tr 標籤，並使其形成一個 list。\n",
    "td_list = airitilibrary_bs4.find_all(\"td\", class_ = \"titleB\")\n",
    "\n",
    "# 使用 for-loop 將每一篇文章的網址、作者、引用次數 等資訊，全部抓取下來。\n",
    "# 每一文章的內容，都會一層一層的p標籤包裹。\n",
    "\n",
    "td_content_list = []\n",
    "for td in td_list : \n",
    "    # 抓取論文標題\n",
    "    td_title = td.find_all(\"p\")[0].text\n",
    "    # print(td_title) # 義守大學《人文與社會學報》第三卷第一期編後語\n",
    "\n",
    "    # 抓取論文網址(可連結至單篇論文之網址)\n",
    "    try : \n",
    "        td_url = \"https://www.airitilibrary.com/\" + td.find_all(\"p\")[0].find(\"a\")[\"href\"]\n",
    "        # print(td_url)\n",
    "    except Exception as e : \n",
    "        td_url = numpy.nan\n",
    "        with open(os_path([\"error\", \"error_crawler_step1\", \"url_error\"])) as f : \n",
    "            # 如果是用 for-loop 去處理，這裡可能還會需要寫入哪一頁有問題需要修正。 \n",
    "            f.write(traceback.format_exc() + \"\\n\")\n",
    "\n",
    "    # 抓取論文作者\n",
    "    try : \n",
    "        td_author = td.find_all(\"p\")[1].find(\"a\").text\n",
    "        # print(td_author) # 義守大學《人文與社會學報》編輯小組\n",
    "    except : \n",
    "        td_author = numpy.nan\n",
    "        with open(os_path([\"error\", \"error_crawler_step1\", \"author_error\"])) as f : \n",
    "            # 如果是用 for-loop 去處理，這裡可能還會需要寫入哪一頁有問題需要修正。 \n",
    "            f.write(traceback.format_exc() + \"\\n\")\n",
    "\n",
    "    # 將內容都裝到一個 dict 中，之後輸出成 excel。\n",
    "    td_content = {\n",
    "        \"td_title\" : td_title,\n",
    "        \"td_url\" : td_url,\n",
    "    }\n",
    "    td_content_list.append(td_content)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_content_df = pandas.DataFrame(td_content_list)\n",
    "td_content_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_2\n",
    "此處對每一個蒐集到的 url 發送請求，並蒐集作者、文章摘要等資訊。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建錯誤資訊資料夾\n",
    "os.makedirs(os_path([\"程式碼\", \"airitilibrary_cralwer\", \"error\", \"error_crawler_step2\"]), mode=0o777, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對 td_url 之中，每一個網址發出請求。\n",
    "url_content_list = []\n",
    "for url in list(dict.fromkeys(td_content_df[\"td_url\"])) :\n",
    "\n",
    "    # 如果無法成功發送請求，則將其資訊另外蒐集起來。蒐集 title、url。\n",
    "    try : \n",
    "        url_request = requests.get(url)\n",
    "        url_bs4 = BeautifulSoup(url_request.text)\n",
    "\n",
    "    except Exception as e : \n",
    "        url_title = td_content_df[td_content_df[\"td_url\"] == url][\"td_title\"][0]\n",
    "        error_element = {\"td_title\" : url_title, \"td_url\" : url, \"error_information\" : traceback.format_exc()}\n",
    "        print(error_element)\n",
    "        continue\n",
    "    \n",
    "    # 請求發送成功，接著爬取內容，並最終儲存為 CSV 檔案。\n",
    "    # 作者、期刊名稱 ... ...。\n",
    "    try : \n",
    "        div_detail = url_bs4.find(\"div\", class_ = \"detail\")\n",
    "        # print(div_detail)\n",
    "    except : \n",
    "        print(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "    # 作者\n",
    "    try : \n",
    "        author = numpy.nan\n",
    "        p_3 = div_detail.find_all(\"p\")[2].text\n",
    "        author = re.sub(\"[ \\r\\n]\", \"\", p_3) # 正則表達式，替換字串內容。\n",
    "        author = re.sub(\"[；]\", \"、\", author)\n",
    "\n",
    "    except : \n",
    "        print(traceback.format_exc())\n",
    "    \n",
    "    # 論文名稱\n",
    "    try : \n",
    "        paper_name = numpy.nan\n",
    "        p_1 = div_detail.find_all(\"p\")[0].text\n",
    "        paper_name = re.sub(\"[\\r\\n ；]\", \"\", p_1) # 設計社會學\n",
    "    except : \n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    # 期刊名稱\n",
    "    try : \n",
    "        periodicals_name = numpy.nan\n",
    "        p_4 = div_detail.find_all(\"p\")[3].text.split(\"；\")[0]\n",
    "        periodicals_name = re.sub(\"[\\r\\n ；]\", \"\", p_4) # 設計研究\n",
    "    except : \n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    # 發表日期\n",
    "    try : \n",
    "        periodicals_date = numpy.nan\n",
    "        p_4 = re.search(\"\\d{4}\", div_detail.find_all(\"p\")[3].text.split(\"；\")[1]).group(0)\n",
    "        periodicals_date = re.sub(\"[\\r\\n ；]\", \"\", p_4) # 2007\n",
    "    except : \n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    # 發表卷數期數\n",
    "    try : \n",
    "        periodicals_detail = numpy.nan\n",
    "        p_4 = re.search(\"\\w+期\", div_detail.find_all(\"p\")[3].text.split(\"；\")[1]).group(0)\n",
    "        periodicals_detail = re.sub(\"[\\r\\n ；]\", \"\", p_4) # 7期\n",
    "    except : \n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    # 發表頁數\n",
    "    try : \n",
    "        periodicals_page = numpy.nan\n",
    "        p_4 = re.search(\"P.*\", div_detail.find_all(\"p\")[3].text.split(\"；\")[1]).group(0)\n",
    "        periodicals_page = re.sub(\"[\\r\\n ；]\", \"\", p_4) # P42 - 48\n",
    "    except : \n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    # 摘要、引用部分\n",
    "    try : \n",
    "        div_Booksainly1 = numpy.nan\n",
    "        div_centerDowm = url_bs4.find(\"div\", class_ = \"centerDowm\")\n",
    "        div_Booksainly1 = div_centerDowm.find(\"div\", \"Booksainly1\")\n",
    "    except : \n",
    "        print(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "    # 摘要\n",
    "    try : \n",
    "        abstract = numpy.nan\n",
    "        textarea_TextArea1 = div_Booksainly1.find(\"textarea\", \"TextArea1\").text\n",
    "        abstract = textarea_TextArea1\n",
    "    except : \n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    # 被引用次數 (有可能文章沒有被引用過)\n",
    "    try : \n",
    "        citations = numpy.nan\n",
    "        div_longMeshTextTitle_2 = div_Booksainly1.find_all(\"div\", \"longMeshTextTitle\")[1].text\n",
    "        div_longMeshTextTitle_2 = re.search(\"\\d+\", div_longMeshTextTitle_2).group(0)\n",
    "        citations = re.sub(\"[\\r\\n ；]\", \"\", div_longMeshTextTitle_2)\n",
    "\n",
    "    except : \n",
    "        # print(traceback.format_exc())\n",
    "        pass\n",
    "    \n",
    "    url_content = {\"author\" : author, \"paper_name\" : paper_name, \"periodicals_name\" : periodicals_name,\n",
    "                   \"periodicals_detail\" : periodicals_detail, \"periodicals_date\" : periodicals_date, \n",
    "                   \"periodicals_page\" : periodicals_page, \"abstract\" : abstract, \"citations\" : citations\n",
    "                   }\n",
    "    url_content_list.append(url_content)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_content_df = pandas.DataFrame(url_content_list)\n",
    "url_content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸出檔案\n",
    "url_content_df.to_csv(os_path([\"database\", \"華藝資料庫.csv\"]), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sigolon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "281f96fdd8214c9ed270da771002bdb3b38d3e1ea124812b79291dd4582c3fdf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
